<h1 align="center" style="line-height: 50px;">
  <img src='https://merit-2025.github.io/static/images/merit.png' width="4%" height="auto" style="vertical-align: middle;">
  MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query
</h1>

<div align="center">
Wei Chow<sup>1</sup>*, Yuan Gao<sup>1</sup>*, Linfeng Li<sup>1</sup>*, Xian Wang<sup>1</sup>, Qi Xu<sup>1</sup>, Hang Song<sup>1</sup>, Lingdong Kong<sup>1</sup>, 

Ran Zhou<sup>1</sup>, Yi Zeng<sup>1</sup>, Yidong Cai<sup>1</sup>, Botian Jiang<sup>1</sup>, Shilin Xu<sup>1</sup>, Jiajun Zhang<sup>1</sup>, Minghui Qiu<sup>1</sup>, Xiangtai Li<sup>1</sup>, Tianshu Yang<sup>1</sup>, Siliang Tang<sup>2</sup>, Juncheng Li<sup>2</sup>

<sup>1</sup>Bytedance, <sup>2</sup>Zhejiang University    <small>*Equal Contribution</small>

[![arXiv](https://img.shields.io/badge/arXiv-2506.03144-b31b1b.svg)](https://arxiv.org/abs/2506.03144)
[![Dataset](https://img.shields.io/badge/ü§ó%20Huggingface-Dataset-yellow)](https://huggingface.co/datasets/WeiChow/merit)
[![Checkpoint](https://img.shields.io/badge/ü§ó%20Huggingface-CKPT-blue)]()
[![GitHub](https://img.shields.io/badge/GitHub-Repo-181717?logo=github)](https://github.com/weichow23/merit)
[![Page](https://img.shields.io/badge/Home-Page-b3.svg)](https://merit-2025.github.io//)
</div>

## üöÄ Project Introduction

![](https://merit-2025.github.io/static/images/teaser.png)

Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.


## üìä MERIT

### üìù Annotation
![](https://merit-2025.github.io/static/images/part1/pipeline.png)

We open source some of the key code for annotation mentioned in our paper, in the `merit` folder.

```bash
conda create --name merit python=3.9.2
conda activate merit
pip3 install -r requirements.txt
pip3 install --upgrade deepspeed transformers torch
pip3 install git+https://github.com/openai/CLIP.git
```

Specifically, the process is as follows:

```python
# 1Ô∏è‚É£ Using the results of open set labeling, select representative key-values
python3 merit/select_key_value.py
# For the key-values ‚Äã‚Äãgenerated by the previous script, we further use GPT to filter out usable key-values. The results are shown in the file `merit/attribute_cards`
# 2Ô∏è‚É£ Using these reduced key-values, we can annotate attribute key-values ‚Äã‚Äãfor each product (note that one SPU has multiple SKUs)
python3 merit/annotate_spu.py
# 3Ô∏è‚É£-1 After annotating the products, combine the products to form query pairs
python3 merit/compose_query.py    # 1. Generate query by combining products
python3 merit/annotate_query.py   # 2. Generate personalized instructions for query
python3 merit/filter_query.py     # 3. Mechanically filter query
# 3Ô∏è‚É£-2 Produce directly from similar products instead of combining them first
python3 merit/compose_query_cold.py # Generate query directly from similar products
# 4Ô∏è‚É£ Manual labeling and filtering
# üìä For the division of OOD data, see merit/exp_ood_split.py
```

### üõ†Ô∏è Usage

1Ô∏è‚É£ Download the dataset

The dataset can be download in [![Dataset](https://img.shields.io/badge/ü§ó%20Huggingface-Dataset-yellow)](https://huggingface.co/datasets/WeiChow/merit).

```shell
huggingface-cli download WeiChow/merit --repo-type dataset --local-dir <YOUR SAVE DIR> --local-dir-use-symlinks False
```

Then you can use the dataset directly.

2Ô∏è‚É£ Load the dataset

```python
from datasets import load_dataset
from tqdm import tqdm
# https://github.com/weichow23/merit/blob/main/annotator/utils.py
from annotator.utils import read_json_data

# if you download the merit in the default huggingface path you can use "WeiChow/merit" instead of <YOUR SAVE DIR>
train_products = load_dataset("WeiChow/merit")["train"]
test_products = load_dataset("WeiChow/merit")["test"]

train_queries = read_json_data(f"{<YOUR SAVE DIR>}/queries-train.json")
test_queries = read_json_data(f"{<YOUR SAVE DIR>}/queries-test.json")
```

3Ô∏è‚É£ How to use the dataset (use test set as example)

```python
# Create an inverted index table for products"
# It may cost some time, if you want to accelerate:
# I suggest you store the image in test_products locally and change the field to the local image address, 
# and then read it. This is convenient and does not take a minute.
product_map = {p["idx"]: p for p in tqdm(test_products, desc="Creating product map")}

for item in tqdm(test_queries):
  print(item)
  # query instruction
  print(item["query instruction"])
  # query product
  for q in item['query']:
    # image, title, idx, class, country, language, attribute
    q_product = product_map[str(q)]
    print(q_product['image'])
    print(q_product['title'])
  # candidate product
  for c in item ['pos_candidate']:
    c_product = product_map[str(c)]
    print(c_product['image'])
    print(c_product['title'])
  break
```

4Ô∏è‚É£ Calculate the metrics

```python
# https://github.com/weichow23/merit/blob/main/annotator/utils.py
from annotator.utils import calculate_mrr

# After inference is completed, save the result as a dict in the following format
# Case: result_dict = {"1": -1, "2": -1, "3": 2, "4": -1, "5": 7}
# 1,2,3,4,5 are the idx of the query, and the corresponding value is the position where the first positive sample appears 
# (if there is more than one positive sample, the one with a smaller value is taken, that is, the one in front), 
# if > 10, it is -1
calculate_mrr(result_dict)
```

## ü™∏ Coral
### üåü Introduction
Recognizing neglecting specific conditional elements in queries as a primary source of error, we introduce LogoCoral to enhance MLLM-based retriever performance in addressing interleaved multi-condition semantic retrieval tasks through the integration of visual reconstruction during the fine-tuning process of the MLLM-to-retrieval model adaptation.

![](https://merit-2025.github.io/static/images/part3/method.png)

### üî• Quick Start
Coming soon!

1Ô∏è‚É£ Set up environment


2Ô∏è‚É£ Download pretrained checkpoint


3Ô∏è‚É£ Eval

### üèãÔ∏è Training
Coming soon!

## ‚úçÔ∏è Citation
```
@article{chow2025merit,
  title={MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query},
  author={Chow, Wei and Gao, Yuan and Li, Linfeng and Wang, Xian and Xu, Qi and Song, Hang and Kong, Lingdong and Zhou, Ran and Zeng, Yi and Cai, Yidong and others},
  journal={arXiv preprint arXiv:2506.03144},
  year={2025}
}
```

## üìú License
MERIT is licensed under the Apache 2.0.
